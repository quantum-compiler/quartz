{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f06b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch[07:53:46] /opt/dgl/src/runtime/tensordispatch.cc:\n",
      "43: TensorDispatcher: dlopen failed: /home/zikunli/anaconda3/envs/quantum/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "import quartz\n",
    "quartz_context = quartz.QuartzContext(gate_set=['h', 'cx', 'x', 't', 'tdg', 'ccz'], filename='../bfs_verified_simplified.json')\n",
    "parser = quartz.PyQASMParser(context=quartz_context)\n",
    "# my_dag = parser.load_qasm(filename=\"../circuit/voqc-benchmarks/tof_4.qasm\")\n",
    "# my_dag = parser.load_qasm(filename=\"../circuit/nam-circuits/qasm_files/tof_4_before.qasm\")\n",
    "my_dag = parser.load_qasm(filename=\"barenco_tof_3_opt_path/subst_history_39.qasm\")\n",
    "init_graph = quartz.PyGraph(context=quartz_context, dag=my_dag)\n",
    "print(init_graph.num_nodes)\n",
    "# init_graph = init_graph.toffoli_flip(context=quartz_context, target=\"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15f25615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "gate_type_num = 26\n",
    "class QAgent:\n",
    "    def __init__(self, lr, gamma,a_size, pretrained_model=None):\n",
    "        torch.manual_seed(42)\n",
    "        if pretrained_model != None:\n",
    "            self.q_net = copy.deepcopy(pretrained_model)\n",
    "        else:\n",
    "            self.q_net = QGNN(gate_type_num, 16, a_size, 16)\n",
    "        self.target_net = copy.deepcopy(self.q_net)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr = lr)\n",
    "        self.a_size = a_size \n",
    "        self.gamma = gamma\n",
    "               \n",
    "    def select_a(self, g, dgl_g, e):\n",
    "        a_size = self.a_size\n",
    "        \n",
    "        if random.random() < e:\n",
    "            node = np.random.randint(0, dgl_g.num_nodes())\n",
    "            # A = np.random.randint(0, a_size)\n",
    "            xfers = g.available_xfers(context=quartz_context, node=g.get_node_from_id(id=node))\n",
    "            if xfers != []:\n",
    "                A = random.choice(xfers)\n",
    "            else:\n",
    "                A = np.random.randint(0, a_size)\n",
    "            A = torch.tensor(A)\n",
    "            node = torch.tensor(node)\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = self.q_net(dgl_g)\n",
    "            Qs, As = torch.max(pred, dim=1)\n",
    "            Q, node = torch.max(Qs, dim = 0)\n",
    "            A = As[node]                 \n",
    "        \n",
    "        return node, A\n",
    "    \n",
    "\n",
    "    def train(self, data, batch_size):\n",
    "        losses = 0\n",
    "        pred_rs = []\n",
    "        target_rs = []\n",
    "        for i in range(batch_size):\n",
    "            s, node, a, r, s_next = data.get_data()\n",
    "\n",
    "            pred = self.q_net(s)\n",
    "            pred_r = pred[node][a]\n",
    "            #s_a = s_as.gather(1, a)\n",
    "\n",
    "            if s_next == None:\n",
    "                target_r = torch.tensor(-1.0)\n",
    "            else:\n",
    "                # TODO: modify this part\n",
    "                q_next = self.target_net(s_next).detach()\n",
    "                q_next_r = q_next[node][a]\n",
    "                # print(q_next.shape)\n",
    "                target_r = r + self.gamma * q_next_r\n",
    "            \n",
    "            pred_rs.append(pred_r)\n",
    "            target_rs.append(target_r)\n",
    "        loss = self.loss_fn(torch.stack(pred_rs), torch.stack(target_rs))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        for param in self.q_net.parameters():\n",
    "            param.grad.data.clamp_(-1,1)\n",
    "        self.optimizer.step()\n",
    "              \n",
    "        return loss.item()    \n",
    "\n",
    "    \n",
    "class QData:\n",
    "    def __init__(self):\n",
    "        self.data = deque(maxlen=100000)\n",
    "        \n",
    "    def add_data(self, d):\n",
    "        self.data.append(d)\n",
    "        \n",
    "    def get_data(self):\n",
    "        s = random.sample(self.data, 1)[0]\n",
    "        #print(s)\n",
    "        return s[0],s[1],s[2],s[3],s[4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f168484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "\n",
    "class QConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_feat, inter_dim, out_feat):\n",
    "        super(QConv, self).__init__()\n",
    "        self.linear2 = nn.Linear(in_feat + inter_dim, out_feat)\n",
    "        self.linear1 = nn.Linear(in_feat + 3, inter_dim, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.linear1.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.linear2.weight, gain=gain)\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        #print(f'node h {edges.src[\"h\"].shape}')\n",
    "        #print(f'node w {edges.data[\"w\"].shape}')\n",
    "        return {'m': torch.cat([edges.src['h'], edges.data['w']], dim=1)}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        #print(f'node m {nodes.mailbox[\"m\"].shape}')\n",
    "        tmp = self.linear1(nodes.mailbox['m'])\n",
    "        tmp = F.leaky_relu(tmp)\n",
    "        h = torch.mean(tmp, dim=1)\n",
    "        return {'h_N': h}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        g.ndata['h'] = h\n",
    "        #g.edata['w'] = w #self.embed(torch.unsqueeze(w,1))\n",
    "        g.update_all(self.message_func, self.reduce_func)\n",
    "        h_N = g.ndata['h_N']\n",
    "        h_total = torch.cat([h, h_N], dim=1)\n",
    "        return self.linear2(h_total)\n",
    "\n",
    "\n",
    "\n",
    "class QGNN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes, inter_dim):\n",
    "        super(QGNN, self).__init__()\n",
    "        self.conv1 = QConv(in_feats, inter_dim, h_feats)\n",
    "        self.conv2 = QConv(h_feats, inter_dim, h_feats)\n",
    "        self.conv3 = QConv(h_feats, inter_dim, h_feats)\n",
    "        self.conv4 = QConv(h_feats, inter_dim, h_feats)\n",
    "        self.conv5 = QConv(h_feats, inter_dim, num_classes)\n",
    "        self.embedding = nn.Embedding(in_feats, in_feats)\n",
    "    \n",
    "    def forward(self, g):\n",
    "        #print(g.ndata['gate_type'])\n",
    "        #print(self.embedding)\n",
    "        g.ndata['h'] = self.embedding(g.ndata['gate_type'])\n",
    "        w = torch.cat([torch.unsqueeze(g.edata['src_idx'],1),torch.unsqueeze(g.edata['dst_idx'],1),torch.unsqueeze(g.edata['reversed'],1)],dim = 1)\n",
    "        g.edata['w'] = w \n",
    "        h = self.conv1(g, g.ndata['h'])\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv3(g, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv4(g, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv5(g, h)\n",
    "        # h = F.softmax(h)\n",
    "        return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "819ecf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretraining using tof_3 circuit\n",
    "# Preparaing data\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def get_dataset(i):\n",
    "    dag_i = parser.load_qasm(filename=\"barenco_tof_3_opt_path/subst_history_\" + str(i) + \".qasm\")\n",
    "    graph = quartz.PyGraph(context=quartz_context, dag=dag_i)\n",
    "    dgl_graph = graph.to_dgl_graph()\n",
    "    appliable_xfer_matrix = graph.get_available_xfers_matrix(context=quartz_context)\n",
    "    dgl_graph.ndata['label'] = torch.tensor(appliable_xfer_matrix,dtype=torch.float)\n",
    "    return dgl_graph\n",
    "\n",
    "idx_list = list(range(40))\n",
    "with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    results = executor.map(get_dataset, idx_list)\n",
    "\n",
    "opt_path_dgls = [r for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6a7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(input, target, weight):\n",
    "    return (weight * (input - target) ** 2).mean()\n",
    "\n",
    "def train_supervised(g, model, lr=0.01, epochs=20):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    all_logits = []\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['gate_type']\n",
    "\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    weight = labels[train_mask] == 1\n",
    "    for e in range(epochs):\n",
    "        # Forward\n",
    "        logits = model(g)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that we should only compute the losses of the nodes in the training set,\n",
    "        # i.e. with train_mask 1.\n",
    "        #print(logits)\n",
    "        \n",
    "        loss = torch.nn.MSELoss()(logits[train_mask], labels[train_mask])\n",
    "        # loss = torch.nn.CrossEntropyLoss()(logits[train_mask], labels[train_mask])\n",
    "        pred = logits > 0.5\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        train_recall = torch.sum((torch.logical_and((pred[train_mask] == 1), (labels[train_mask] == 1))).float()) / torch.sum((labels[train_mask] == 1).float())\n",
    "        val_recall = torch.sum((torch.logical_and((pred[val_mask] == 1), (labels[val_mask] == 1))).float()) / torch.sum((labels[val_mask] == 1).float())\n",
    "        test_recall = torch.sum((torch.logical_and((pred[test_mask] == 1), (labels[test_mask] == 1))).float()) / torch.sum((labels[test_mask] == 1).float())\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_logits.append(logits.detach())\n",
    "\n",
    "        # TODO: print out false negative\n",
    "        if e % 1 == 0:\n",
    "            print('In epoch {}, loss: {:.5f}, train acc: {:.5f}, train recall: {:.5f}, val acc: {:.5f} (best {:.5f}), val recall: {:.5f}, test acc: {:.5f} (best {:.5f}), test recall: {:.5f}'.format(\n",
    "                e, loss, train_acc, train_recall, val_acc, best_val_acc,val_recall, test_acc, best_test_acc, test_recall))\n",
    "\n",
    "def test(*, filename):\n",
    "    test_dag = parser.load_qasm(filename=filename)\n",
    "    test_graph = quartz.PyGraph(context=quartz_context, dag=test_dag)\n",
    "    test_graph_dgl = test_graph.to_dgl_graph()\n",
    "    appliable_xfer_matrix = test_graph.get_available_xfers_matrix(context=quartz_context)\n",
    "    test_graph_dgl.ndata['label'] = torch.tensor(appliable_xfer_matrix,dtype=torch.float)\n",
    "    labels = test_graph_dgl.ndata['label']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(test_graph_dgl)\n",
    "        pred = logits > 0.5\n",
    "        test_acc = (pred == labels).float().mean()\n",
    "        print(f\"test_acc: {test_acc:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b091986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.21373, train acc: 0.86040, train recall: 0.25383, val acc: 0.86115 (best 0.86115), val recall: 0.25666, test acc: 0.86047 (best 0.86047), test recall: 0.24821\n",
      "In epoch 1, loss: 3676.76221, train acc: 0.49241, train recall: 0.52089, val acc: 0.49248 (best 0.86115), val recall: 0.53447, test acc: 0.49243 (best 0.86047), test recall: 0.52242\n",
      "In epoch 2, loss: 3.67248, train acc: 0.52466, train recall: 0.45903, val acc: 0.52473 (best 0.86115), val recall: 0.44642, test acc: 0.52465 (best 0.86047), test recall: 0.45809\n",
      "In epoch 3, loss: 1.73481, train acc: 0.65387, train recall: 0.41338, val acc: 0.66081 (best 0.86115), val recall: 0.40273, test acc: 0.65666 (best 0.86047), test recall: 0.41520\n",
      "In epoch 4, loss: 11.09173, train acc: 0.50127, train recall: 0.51896, val acc: 0.50179 (best 0.86115), val recall: 0.53174, test acc: 0.50092 (best 0.86047), test recall: 0.51982\n",
      "In epoch 5, loss: 168.47055, train acc: 0.51202, train recall: 0.47955, val acc: 0.51221 (best 0.86115), val recall: 0.46485, test acc: 0.51210 (best 0.86047), test recall: 0.47628\n",
      "In epoch 6, loss: 0.81669, train acc: 0.55348, train recall: 0.45071, val acc: 0.55720 (best 0.86115), val recall: 0.43686, test acc: 0.55323 (best 0.86047), test recall: 0.45094\n",
      "In epoch 7, loss: 6.39516, train acc: 0.60022, train recall: 0.40610, val acc: 0.59324 (best 0.86115), val recall: 0.40478, test acc: 0.59475 (best 0.86047), test recall: 0.40221\n",
      "In epoch 8, loss: 10.62385, train acc: 0.49183, train recall: 0.52089, val acc: 0.49186 (best 0.86115), val recall: 0.53447, test acc: 0.49183 (best 0.86047), test recall: 0.52242\n",
      "In epoch 9, loss: 7.64470, train acc: 0.52571, train recall: 0.46305, val acc: 0.52177 (best 0.86115), val recall: 0.45051, test acc: 0.52455 (best 0.86047), test recall: 0.46004\n",
      "In epoch 10, loss: 5.17350, train acc: 0.62281, train recall: 0.32342, val acc: 0.63325 (best 0.86115), val recall: 0.32082, test acc: 0.61720 (best 0.86047), test recall: 0.32424\n",
      "In epoch 11, loss: 3.25239, train acc: 0.52095, train recall: 0.47911, val acc: 0.52292 (best 0.86115), val recall: 0.46553, test acc: 0.52046 (best 0.86047), test recall: 0.47758\n",
      "In epoch 12, loss: 0.81130, train acc: 0.77243, train recall: 0.46974, val acc: 0.77374 (best 0.86115), val recall: 0.46553, test acc: 0.77835 (best 0.86047), test recall: 0.44509\n",
      "In epoch 13, loss: 4.87296, train acc: 0.50156, train recall: 0.52476, val acc: 0.50161 (best 0.86115), val recall: 0.54061, test acc: 0.50162 (best 0.86047), test recall: 0.53346\n",
      "In epoch 14, loss: 0.45959, train acc: 0.77022, train recall: 0.50364, val acc: 0.76643 (best 0.86115), val recall: 0.49488, test acc: 0.76748 (best 0.86047), test recall: 0.47628\n",
      "In epoch 15, loss: 2.97930, train acc: 0.50925, train recall: 0.47911, val acc: 0.50904 (best 0.86115), val recall: 0.46553, test acc: 0.50924 (best 0.86047), test recall: 0.47758\n",
      "In epoch 16, loss: 19.33543, train acc: 0.49556, train recall: 0.52178, val acc: 0.49454 (best 0.86115), val recall: 0.53515, test acc: 0.49514 (best 0.86047), test recall: 0.52307\n",
      "In epoch 17, loss: 13.65711, train acc: 0.51450, train recall: 0.47390, val acc: 0.51455 (best 0.86115), val recall: 0.46212, test acc: 0.51465 (best 0.86047), test recall: 0.47563\n",
      "In epoch 18, loss: 4.32012, train acc: 0.54701, train recall: 0.43271, val acc: 0.54913 (best 0.86115), val recall: 0.42457, test acc: 0.54657 (best 0.86047), test recall: 0.42950\n",
      "In epoch 19, loss: 2.80227, train acc: 0.63702, train recall: 0.50528, val acc: 0.63553 (best 0.86115), val recall: 0.51263, test acc: 0.62548 (best 0.86047), test recall: 0.50877\n",
      "In epoch 20, loss: 4.77845, train acc: 0.54343, train recall: 0.50379, val acc: 0.54364 (best 0.86115), val recall: 0.51468, test acc: 0.54559 (best 0.86047), test recall: 0.50617\n",
      "In epoch 21, loss: 1.03015, train acc: 0.67468, train recall: 0.50840, val acc: 0.66589 (best 0.86115), val recall: 0.51126, test acc: 0.66713 (best 0.86047), test recall: 0.49578\n",
      "In epoch 22, loss: 2.79231, train acc: 0.52837, train recall: 0.47762, val acc: 0.52981 (best 0.86115), val recall: 0.46553, test acc: 0.53348 (best 0.86047), test recall: 0.47628\n",
      "In epoch 23, loss: 2.13523, train acc: 0.60087, train recall: 0.27836, val acc: 0.60258 (best 0.86115), val recall: 0.29078, test acc: 0.61195 (best 0.86047), test recall: 0.26901\n",
      "In epoch 24, loss: 0.45516, train acc: 0.78074, train recall: 0.23152, val acc: 0.77829 (best 0.86115), val recall: 0.24300, test acc: 0.79471 (best 0.86047), test recall: 0.21378\n",
      "In epoch 25, loss: 0.34771, train acc: 0.78623, train recall: 0.34959, val acc: 0.77880 (best 0.86115), val recall: 0.35904, test acc: 0.78614 (best 0.86047), test recall: 0.33268\n",
      "In epoch 26, loss: 0.31090, train acc: 0.80178, train recall: 0.32015, val acc: 0.79366 (best 0.86115), val recall: 0.32833, test acc: 0.80338 (best 0.86047), test recall: 0.31059\n",
      "In epoch 27, loss: 0.15168, train acc: 0.90634, train recall: 0.22335, val acc: 0.90420 (best 0.90420), val recall: 0.23549, test acc: 0.90740 (best 0.90740), test recall: 0.22287\n",
      "In epoch 28, loss: 0.11588, train acc: 0.93653, train recall: 0.20506, val acc: 0.93583 (best 0.93583), val recall: 0.20478, test acc: 0.93305 (best 0.93305), test recall: 0.19753\n",
      "In epoch 29, loss: 0.13080, train acc: 0.92129, train recall: 0.19227, val acc: 0.92396 (best 0.93583), val recall: 0.18771, test acc: 0.91085 (best 0.93305), test recall: 0.18194\n",
      "In epoch 30, loss: 0.11058, train acc: 0.94095, train recall: 0.17606, val acc: 0.94292 (best 0.94292), val recall: 0.17338, test acc: 0.93202 (best 0.93202), test recall: 0.16569\n",
      "In epoch 31, loss: 0.08116, train acc: 0.96950, train recall: 0.13428, val acc: 0.96886 (best 0.96886), val recall: 0.13038, test acc: 0.96711 (best 0.96711), test recall: 0.12346\n",
      "In epoch 32, loss: 0.07899, train acc: 0.97186, train recall: 0.11405, val acc: 0.96978 (best 0.96978), val recall: 0.11604, test acc: 0.97109 (best 0.97109), test recall: 0.10786\n",
      "In epoch 33, loss: 0.09367, train acc: 0.96541, train recall: 0.10126, val acc: 0.96224 (best 0.96978), val recall: 0.10717, test acc: 0.96487 (best 0.97109), test recall: 0.09097\n",
      "In epoch 34, loss: 0.09717, train acc: 0.96014, train recall: 0.09517, val acc: 0.95682 (best 0.96978), val recall: 0.10375, test acc: 0.95844 (best 0.97109), test recall: 0.08577\n",
      "In epoch 35, loss: 0.08375, train acc: 0.95719, train recall: 0.07197, val acc: 0.95484 (best 0.96978), val recall: 0.08123, test acc: 0.95241 (best 0.97109), test recall: 0.06303\n",
      "In epoch 36, loss: 0.07365, train acc: 0.95821, train recall: 0.04164, val acc: 0.95716 (best 0.96978), val recall: 0.04846, test acc: 0.95062 (best 0.97109), test recall: 0.03769\n",
      "In epoch 37, loss: 0.07547, train acc: 0.96062, train recall: 0.05829, val acc: 0.96073 (best 0.96978), val recall: 0.06621, test acc: 0.95347 (best 0.97109), test recall: 0.05263\n",
      "In epoch 38, loss: 0.07770, train acc: 0.96109, train recall: 0.05487, val acc: 0.96224 (best 0.96978), val recall: 0.06075, test acc: 0.95451 (best 0.97109), test recall: 0.05393\n",
      "In epoch 39, loss: 0.07106, train acc: 0.96511, train recall: 0.05651, val acc: 0.96601 (best 0.96978), val recall: 0.06007, test acc: 0.95797 (best 0.97109), test recall: 0.05523\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "bg = dgl.batch(opt_path_dgls)\n",
    "node_cnt = bg.num_nodes()\n",
    "l = list(range(node_cnt))\n",
    "train_rate = 0.7\n",
    "val_rate = 0.15\n",
    "\n",
    "train_num = int(node_cnt * train_rate)\n",
    "val_num = int(node_cnt * val_rate)\n",
    "test_num = node_cnt - train_num - val_num\n",
    "\n",
    "train_sample = sample(l, train_num)\n",
    "node_left = [n for n in l if n not in train_sample]\n",
    "val_sample = sample(node_left, val_num)\n",
    "test_sample = [n for n in node_left if n not in val_sample]\n",
    "\n",
    "train_mask = [0] * node_cnt\n",
    "val_mask = [0] * node_cnt\n",
    "test_mask = [0] * node_cnt\n",
    "\n",
    "for i in range(node_cnt):\n",
    "    if i in train_sample:\n",
    "        train_mask[i] = 1\n",
    "    elif i in val_sample:\n",
    "        val_mask[i] = 1\n",
    "    elif i in test_sample:\n",
    "        test_mask[i] = 1\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "bg.ndata['train_mask'] = torch.tensor(train_mask,dtype=torch.bool) \n",
    "bg.ndata['val_mask'] = torch.tensor(val_mask,dtype=torch.bool) \n",
    "bg.ndata['test_mask'] = torch.tensor(test_mask,dtype=torch.bool) \n",
    "\n",
    "model = QGNN(26, 128, quartz_context.num_xfers, 128)\n",
    "train_supervised(bg, model, lr=0.04, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0608e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted reward\n",
    "dag_39 = parser.load_qasm(filename=\"barenco_tof_3_opt_path/subst_history_39.qasm\")\n",
    "graph_39 = quartz.PyGraph(context=quartz_context, dag=dag_39)\n",
    "dgl_graph_39 = graph_39.to_dgl_graph()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(dgl_graph_39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76631647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  [3259, 3355]\n",
      "1 :  [3337, 3461]\n",
      "2 :  [3321, 3345, 3385]\n",
      "3 :  [1725, 3337, 3461]\n",
      "4 :  [3363]\n",
      "5 :  [1907, 3337, 3461]\n",
      "6 :  [368, 1351, 2098, 3321, 3345, 3385]\n",
      "7 :  [398, 440, 441, 442, 444, 1091, 1172, 1231, 1232, 1233, 1235, 1471, 1472, 1473, 1475, 3337, 3461]\n",
      "8 :  [3287, 3288, 3337, 3430, 3461, 3490]\n",
      "9 :  [3363, 3583]\n",
      "10 :  [3321, 3345, 3385]\n",
      "11 :  [3224, 3337, 3360, 3461]\n",
      "12 :  [3223, 3359]\n",
      "13 :  [3241]\n",
      "14 :  [3259, 3355]\n",
      "15 :  [3337, 3461]\n",
      "16 :  [3321, 3345, 3385]\n",
      "17 :  [1725, 3337, 3461]\n",
      "18 :  [3363]\n",
      "19 :  [1907, 3337, 3461]\n",
      "20 :  [368, 1351, 2098, 3321, 3345, 3385]\n",
      "21 :  [398, 440, 441, 442, 444, 1091, 1172, 1231, 1232, 1233, 1235, 1471, 1472, 1473, 1475, 3337, 3461]\n",
      "22 :  [3287, 3288, 3337, 3430, 3461, 3490]\n",
      "23 :  []\n",
      "24 :  [2613, 3321, 3345, 3385]\n",
      "25 :  [3291, 3383]\n",
      "26 :  [1205, 1477, 1817, 2105, 3224, 3337, 3360, 3461]\n",
      "27 :  [1725, 3337, 3461]\n",
      "28 :  [3223, 3359]\n",
      "29 :  [3223, 3359]\n",
      "30 :  [3363]\n",
      "31 :  [3337, 3461]\n",
      "32 :  [3321, 3345, 3385]\n",
      "33 :  [1557, 1559, 1725, 3337, 3461]\n",
      "34 :  [1241, 3363]\n",
      "35 :  [440, 441, 442, 444, 1231, 1232, 1233, 1235, 1281, 1471, 1472, 1473, 1475, 1815, 2595, 3337, 3461]\n",
      "36 :  [3123, 3124, 3210, 3326, 3337, 3461]\n",
      "37 :  []\n",
      "38 :  [3363, 3511]\n",
      "39 :  []\n",
      "40 :  [3155, 3309, 3337, 3461]\n",
      "41 :  []\n",
      "42 :  [3131]\n",
      "43 :  [3259, 3355]\n",
      "44 :  [1725, 3337, 3461]\n",
      "45 :  [3363]\n",
      "46 :  [3337, 3461]\n",
      "47 :  [3321, 3345, 3385]\n",
      "48 :  [1557, 1559, 1725, 3337, 3461]\n",
      "49 :  [1241, 3363]\n",
      "50 :  [440, 441, 442, 444, 1231, 1232, 1233, 1235, 1281, 1471, 1472, 1473, 1475, 1815, 2595, 3337, 3461]\n",
      "51 :  [3123, 3124, 3210, 3326, 3337, 3461]\n",
      "52 :  []\n",
      "53 :  [3363, 3511]\n",
      "54 :  []\n",
      "55 :  [3155, 3309, 3337, 3461]\n",
      "56 :  []\n",
      "57 :  []\n"
     ]
    }
   ],
   "source": [
    "# Get predicted reward for certain action\n",
    "\n",
    "all_nodes = graph_39.all_nodes()\n",
    "i = 0\n",
    "for node in all_nodes:\n",
    "    print(i, \": \", graph_39.available_xfers(context=quartz_context, node=node))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddc5795b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0210)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0][398]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db1220b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_56905/2382420928.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mdgl_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dgl_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdgl_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;31m# print(A)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mnew_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_xfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquartz_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xfer_from_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_56905/3849871143.py\u001b[0m in \u001b[0;36mselect_a\u001b[0;34m(self, g, dgl_g, e)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdgl_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# A = np.random.randint(0, a_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mxfers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_xfers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquartz_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_node_from_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mxfers\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "f = open(\"RL.txt\", \"w\")\n",
    "\n",
    "# TODO: change gamma\n",
    "agent = QAgent(lr = 1e-3, gamma = 0.99, a_size = 3397, pretrained_model = model)\n",
    "data = QData()\n",
    "\n",
    "# More episodes\n",
    "replay_times = 1000\n",
    "episodes = 20\n",
    "epsilon = 0.3\n",
    "train_epoch = 5\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    rewards = 0\n",
    "    losses = 0\n",
    "    for j in range(replay_times):\n",
    "        count = 0\n",
    "        end = False\n",
    "        g = init_graph\n",
    "        while(count < 20 and not end):\n",
    "            dgl_g = g.to_dgl_graph()\n",
    "            count += 1 \n",
    "            node, A = agent.select_a(g, dgl_g, epsilon)\n",
    "            # print(A)\n",
    "            new_g = g.apply_xfer(xfer=quartz_context.get_xfer_from_id(id=A), node = g.all_nodes()[node])\n",
    "            \n",
    "            if new_g == None:\n",
    "                end = True\n",
    "                data.add_data([dgl_g, node, A, torch.tensor(-1), None])\n",
    "            \n",
    "            else:\n",
    "                dgl_new_g = new_g.to_dgl_graph()\n",
    "                reward = g.gate_count - new_g.gate_count\n",
    "                                         \n",
    "                data.add_data([dgl_g, node, A, torch.tensor(reward), dgl_new_g])\n",
    "            \n",
    "                g = new_g\n",
    "                rewards += reward\n",
    "                print(str(g.gate_count), file=f)\n",
    "        \n",
    "\n",
    "    for j in range(train_epoch):\n",
    "        loss = agent.train(data, 3)\n",
    "        losses += loss  \n",
    "        \n",
    "    if epsilon > 0.05 :\n",
    "        epsilon -= 0.0001\n",
    "        \n",
    "\n",
    "    agent.target_net.load_state_dict(agent.q_net.state_dict())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
