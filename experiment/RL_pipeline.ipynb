{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f06b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch[00:47:54] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /home/zikunli/anaconda3/envs/quantum/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import quartz\n",
    "quartz_context = quartz.QuartzContext(gate_set=['h', 'cx', 'x', 't', 'tdg'], filename='../bfs_verified_simplified.json')\n",
    "parser = quartz.PyQASMParser(context=quartz_context)\n",
    "my_dag = parser.load_qasm(filename=\"../circuit/nam-circuits/qasm_files/tof_4_before.qasm\")\n",
    "init_graph = quartz.PyGraph(context=quartz_context, dag=my_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15f25615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "gate_type_num = 26\n",
    "class QAgent:\n",
    "    def __init__(self, lr, a_size):\n",
    "        torch.manual_seed(42)\n",
    "        self.q_net = QGNN(gate_type_num, 16, a_size, 16)\n",
    "        self.target_net = copy.deepcopy(self.q_net)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr = lr)\n",
    "        self.a_size = a_size \n",
    "               \n",
    "    def select_a(self, g, e):\n",
    "        a_size = self.a_size\n",
    "        \n",
    "        if random.random() < e:\n",
    "            node = np.random.randint(0, g.num_nodes())\n",
    "            A = np.random.randint(0, a_size)\n",
    "            #print(\"random\")\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = self.q_net(g)\n",
    "            Qs, As = torch.max(pred)\n",
    "            Q, node = torch.max(Qs, dim = 0, keepdim = True)\n",
    "            A = As[node]                 \n",
    "        \n",
    "        #print(node)\n",
    "        #print(A)\n",
    "        return node, A\n",
    "    \n",
    "\n",
    "    def train(self, data, batch_size):\n",
    "        losses = 0\n",
    "        pred_rs = []\n",
    "        target_rs = []\n",
    "        for i in range(batch_size):\n",
    "            s, node, a, r, s_next = data.get_data()\n",
    "\n",
    "            pred = self.q_net(s)\n",
    "            pred_r = pred[node][a]\n",
    "            #s_a = s_as.gather(1, a)\n",
    "\n",
    "            if s_next == None:\n",
    "                target_r = torch.tensor(-1.0)\n",
    "            else:\n",
    "                q_next = self.target_net(s_next).detach()\n",
    "                target_r = r + self.gamma * q_next\n",
    "            \n",
    "            pred_rs.append(pred_r)\n",
    "            target_rs.append(target_r)\n",
    "        loss = self.loss_fn(torch.stack(pred_rs), torch.stack(target_rs))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        for param in self.q_net.parameters():\n",
    "            param.grad.data.clamp_(-1,1)\n",
    "        self.optimizer.step()\n",
    "              \n",
    "        return loss.item()    \n",
    "\n",
    "    \n",
    "class QData:\n",
    "    def __init__(self):\n",
    "        self.data = deque(maxlen=100000)\n",
    "        \n",
    "    def add_data(self, d):\n",
    "        self.data.append(d)\n",
    "        \n",
    "    def get_data(self):\n",
    "        s = random.sample(self.data, 1)[0]\n",
    "        #print(s)\n",
    "        return s[0],s[1],s[2],s[3],s[4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f168484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "\n",
    "class QConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_feat, inter_dim, out_feat):\n",
    "        super(QConv, self).__init__()\n",
    "        self.linear2 = nn.Linear(in_feat + inter_dim, out_feat)\n",
    "        self.linear1 = nn.Linear(in_feat + 3, inter_dim, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.linear1.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.linear2.weight, gain=gain)\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        #print(f'node h {edges.src[\"h\"].shape}')\n",
    "        #print(f'node w {edges.data[\"w\"].shape}')\n",
    "        return {'m': torch.cat([edges.src['h'], edges.data['w']], dim=1)}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        #print(f'node m {nodes.mailbox[\"m\"].shape}')\n",
    "        tmp = self.linear1(nodes.mailbox['m'])\n",
    "        tmp = F.leaky_relu(tmp)\n",
    "        h = torch.mean(tmp, dim=1)\n",
    "        return {'h_N': h}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        g.ndata['h'] = h\n",
    "        #g.edata['w'] = w #self.embed(torch.unsqueeze(w,1))\n",
    "        g.update_all(self.message_func, self.reduce_func)\n",
    "        h_N = g.ndata['h_N']\n",
    "        h_total = torch.cat([h, h_N], dim=1)\n",
    "        return self.linear2(h_total)\n",
    "\n",
    "\n",
    "\n",
    "class QGNN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes, inter_dim):\n",
    "        super(QGNN, self).__init__()\n",
    "        self.conv1 = QConv(in_feats, inter_dim, h_feats)\n",
    "        self.conv2 = QConv(h_feats, inter_dim, h_feats)\n",
    "        self.conv3 = QConv(h_feats, inter_dim, h_feats)\n",
    "        self.conv4 = QConv(h_feats, inter_dim, h_feats)\n",
    "        self.conv5 = QConv(h_feats, inter_dim, num_classes)\n",
    "        self.embedding = nn.Embedding(in_feats, in_feats)\n",
    "    \n",
    "    def forward(self, g):\n",
    "        #print(g.ndata['gate_type'])\n",
    "        #print(self.embedding)\n",
    "        g.ndata['h'] = self.embedding(g.ndata['gate_type'])\n",
    "        w = torch.cat([torch.unsqueeze(g.edata['src_idx'],1),torch.unsqueeze(g.edata['dst_idx'],1),torch.unsqueeze(g.edata['reversed'],1)],dim = 1)\n",
    "        g.edata['w'] = w \n",
    "        h = self.conv1(g, g.ndata['h'])\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv3(g, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv4(g, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv5(g, h)\n",
    "        return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "819ecf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretraining using tof_3 circuit\n",
    "# Preparaing data\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def get_dataset(i):\n",
    "    dag_i = parser.load_qasm(filename=\"tof_3_opt_path/subst_history_\" + str(i) + \".qasm\")\n",
    "    graph = quartz.PyGraph(context=quartz_context, dag=dag_i)\n",
    "    dgl_graph = graph.to_dgl_graph()\n",
    "    appliable_xfer_matrix = graph.get_available_xfers_matrix(context=quartz_context)\n",
    "    dgl_graph.ndata['label'] = torch.tensor(appliable_xfer_matrix,dtype=torch.float)\n",
    "    return dgl_graph\n",
    "\n",
    "idx_list = list(range(40))\n",
    "with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "    results = executor.map(get_dataset, idx_list)\n",
    "\n",
    "opt_path_dgls = [r for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c6a7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised(g, model, lr=0.01, epochs=20):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    all_logits = []\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['gate_type']\n",
    "\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    for e in range(epochs):\n",
    "        # Forward\n",
    "        logits = model(g)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that we should only compute the losses of the nodes in the training set,\n",
    "        # i.e. with train_mask 1.\n",
    "        #print(logits)\n",
    "        \n",
    "        loss = torch.nn.MSELoss()(logits[train_mask], labels[train_mask])\n",
    "        pred = logits > 0.5\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        train_recall = torch.sum((torch.logical_and((pred[train_mask] == 1), (labels[train_mask] == 1))).float()) / torch.sum((labels[train_mask] == 1).float())\n",
    "        val_recall = torch.sum((torch.logical_and((pred[val_mask] == 1), (labels[val_mask] == 1))).float()) / torch.sum((labels[val_mask] == 1).float())\n",
    "        test_recall = torch.sum((torch.logical_and((pred[test_mask] == 1), (labels[test_mask] == 1))).float()) / torch.sum((labels[test_mask] == 1).float())\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_logits.append(logits.detach())\n",
    "\n",
    "        # TODO: print out false negative\n",
    "        if e % 1 == 0:\n",
    "            print('In epoch {}, loss: {:.5f}, train acc: {:.5f}, train recall: {:.5f}, val acc: {:.5f} (best {:.5f}), val recall: {:.5f}, test acc: {:.5f} (best {:.5f}), test recall: {:.5f}'.format(\n",
    "                e, loss, train_acc, train_recall, val_acc, best_val_acc,val_recall, test_acc, best_test_acc, test_recall))\n",
    "\n",
    "def test(*, filename):\n",
    "    test_dag = parser.load_qasm(filename=filename)\n",
    "    test_graph = quartz.PyGraph(context=quartz_context, dag=test_dag)\n",
    "    test_graph_dgl = test_graph.to_dgl_graph()\n",
    "    appliable_xfer_matrix = test_graph.get_available_xfers_matrix(context=quartz_context)\n",
    "    test_graph_dgl.ndata['label'] = torch.tensor(appliable_xfer_matrix,dtype=torch.float)\n",
    "    labels = test_graph_dgl.ndata['label']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(test_graph_dgl)\n",
    "        pred = logits > 0.5\n",
    "        test_acc = (pred == labels).float().mean()\n",
    "        print(f\"test_acc: {test_acc:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b091986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.08711, train acc: 0.95660, train recall: 0.02133, val acc: 0.95812 (best 0.95812), val recall: 0.01309, test acc: 0.95799 (best 0.95799), test recall: 0.01314\n",
      "In epoch 1, loss: 0.04278, train acc: 0.99810, train recall: 0.00690, val acc: 0.99806 (best 0.99806), val recall: 0.00773, test acc: 0.99824 (best 0.99824), test recall: 0.00751\n",
      "In epoch 2, loss: 0.03640, train acc: 0.99845, train recall: 0.08594, val acc: 0.99848 (best 0.99848), val recall: 0.09518, test acc: 0.99855 (best 0.99855), test recall: 0.10701\n",
      "In epoch 3, loss: 0.00817, train acc: 0.99848, train recall: 0.00000, val acc: 0.99850 (best 0.99850), val recall: 0.00000, test acc: 0.99858 (best 0.99858), test recall: 0.00000\n",
      "In epoch 4, loss: 0.00578, train acc: 0.99850, train recall: 0.01117, val acc: 0.99852 (best 0.99852), val recall: 0.01428, test acc: 0.99861 (best 0.99861), test recall: 0.01564\n",
      "In epoch 5, loss: 0.00406, train acc: 0.99860, train recall: 0.07615, val acc: 0.99863 (best 0.99863), val recall: 0.08626, test acc: 0.99872 (best 0.99872), test recall: 0.09324\n",
      "In epoch 6, loss: 0.00853, train acc: 0.99854, train recall: 0.04240, val acc: 0.99857 (best 0.99863), val recall: 0.05354, test acc: 0.99866 (best 0.99872), test recall: 0.05444\n",
      "In epoch 7, loss: 0.00452, train acc: 0.99878, train recall: 0.22017, val acc: 0.99881 (best 0.99881), val recall: 0.23795, test acc: 0.99894 (best 0.99894), test recall: 0.27409\n",
      "In epoch 8, loss: 0.00895, train acc: 0.99856, train recall: 0.23485, val acc: 0.99859 (best 0.99881), val recall: 0.25461, test acc: 0.99873 (best 0.99894), test recall: 0.29161\n",
      "In epoch 9, loss: 0.00378, train acc: 0.99877, train recall: 0.23146, val acc: 0.99880 (best 0.99881), val recall: 0.25223, test acc: 0.99893 (best 0.99894), test recall: 0.28911\n",
      "In epoch 10, loss: 0.00302, train acc: 0.99882, train recall: 0.26371, val acc: 0.99885 (best 0.99885), val recall: 0.28079, test acc: 0.99898 (best 0.99898), test recall: 0.32478\n",
      "In epoch 11, loss: 0.00240, train acc: 0.99882, train recall: 0.26346, val acc: 0.99885 (best 0.99885), val recall: 0.28079, test acc: 0.99897 (best 0.99898), test recall: 0.32416\n",
      "In epoch 12, loss: 0.00201, train acc: 0.99882, train recall: 0.26320, val acc: 0.99885 (best 0.99885), val recall: 0.27960, test acc: 0.99897 (best 0.99898), test recall: 0.32416\n",
      "In epoch 13, loss: 0.00186, train acc: 0.99882, train recall: 0.26320, val acc: 0.99885 (best 0.99885), val recall: 0.27960, test acc: 0.99897 (best 0.99898), test recall: 0.32416\n",
      "In epoch 14, loss: 0.00189, train acc: 0.99882, train recall: 0.26320, val acc: 0.99885 (best 0.99885), val recall: 0.27960, test acc: 0.99897 (best 0.99898), test recall: 0.32416\n",
      "In epoch 15, loss: 0.00194, train acc: 0.99878, train recall: 0.22431, val acc: 0.99881 (best 0.99885), val recall: 0.24509, test acc: 0.99893 (best 0.99898), test recall: 0.27534\n",
      "In epoch 16, loss: 0.00194, train acc: 0.99878, train recall: 0.22243, val acc: 0.99881 (best 0.99885), val recall: 0.24390, test acc: 0.99893 (best 0.99898), test recall: 0.27472\n",
      "In epoch 17, loss: 0.00189, train acc: 0.99879, train recall: 0.22231, val acc: 0.99883 (best 0.99885), val recall: 0.24390, test acc: 0.99894 (best 0.99898), test recall: 0.27472\n",
      "In epoch 18, loss: 0.00181, train acc: 0.99879, train recall: 0.22218, val acc: 0.99883 (best 0.99885), val recall: 0.24390, test acc: 0.99894 (best 0.99898), test recall: 0.27472\n",
      "In epoch 19, loss: 0.00172, train acc: 0.99875, train recall: 0.18969, val acc: 0.99879 (best 0.99885), val recall: 0.21356, test acc: 0.99890 (best 0.99898), test recall: 0.23342\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "bg = dgl.batch(opt_path_dgls)\n",
    "node_cnt = bg.num_nodes()\n",
    "l = list(range(node_cnt))\n",
    "train_rate = 0.7\n",
    "val_rate = 0.15\n",
    "\n",
    "train_num = int(node_cnt * train_rate)\n",
    "val_num = int(node_cnt * val_rate)\n",
    "test_num = node_cnt - train_num - val_num\n",
    "\n",
    "train_sample = sample(l, train_num)\n",
    "node_left = [n for n in l if n not in train_sample]\n",
    "val_sample = sample(node_left, val_num)\n",
    "test_sample = [n for n in node_left if n not in val_sample]\n",
    "\n",
    "train_mask = [0] * node_cnt\n",
    "val_mask = [0] * node_cnt\n",
    "test_mask = [0] * node_cnt\n",
    "\n",
    "for i in range(node_cnt):\n",
    "    if i in train_sample:\n",
    "        train_mask[i] = 1\n",
    "    elif i in val_sample:\n",
    "        val_mask[i] = 1\n",
    "    elif i in test_sample:\n",
    "        test_mask[i] = 1\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "bg.ndata['train_mask'] = torch.tensor(train_mask,dtype=torch.bool) \n",
    "bg.ndata['val_mask'] = torch.tensor(val_mask,dtype=torch.bool) \n",
    "bg.ndata['test_mask'] = torch.tensor(test_mask,dtype=torch.bool) \n",
    "\n",
    "model = QGNN(26, 16, quartz_context.num_xfers, 16)\n",
    "train_supervised(bg, model, lr=0.05, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "db1220b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████▊                                                                                                                                     | 1/10 [00:00<00:01,  7.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766\n",
      "284\n",
      "275\n",
      "266\n",
      "407\n",
      "788\n",
      "216\n",
      "662\n",
      "1078\n",
      "686\n",
      "185\n",
      "356\n",
      "747\n",
      "359\n",
      "713\n",
      "648\n",
      "957\n",
      "859\n",
      "743\n",
      "496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████████████▍                                                                                                       | 3/10 [00:00<00:00,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011\n",
      "531\n",
      "658\n",
      "121\n",
      "713\n",
      "977\n",
      "378\n",
      "370\n",
      "372\n",
      "332\n",
      "379\n",
      "155\n",
      "840\n",
      "197\n",
      "118\n",
      "880\n",
      "875\n",
      "1112\n",
      "624\n",
      "843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████                                                                          | 5/10 [00:00<00:00,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n",
      "235\n",
      "193\n",
      "14\n",
      "714\n",
      "909\n",
      "530\n",
      "64\n",
      "564\n",
      "493\n",
      "663\n",
      "852\n",
      "549\n",
      "846\n",
      "171\n",
      "841\n",
      "672\n",
      "906\n",
      "355\n",
      "78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████▌                                            | 7/10 [00:00<00:00,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493\n",
      "269\n",
      "248\n",
      "1012\n",
      "61\n",
      "589\n",
      "894\n",
      "131\n",
      "983\n",
      "757\n",
      "1082\n",
      "980\n",
      "164\n",
      "1099\n",
      "267\n",
      "640\n",
      "908\n",
      "191\n",
      "1065\n",
      "883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏              | 9/10 [00:01<00:00,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836\n",
      "962\n",
      "457\n",
      "979\n",
      "1048\n",
      "505\n",
      "168\n",
      "362\n",
      "676\n",
      "461\n",
      "364\n",
      "18\n",
      "188\n",
      "416\n",
      "1097\n",
      "381\n",
      "132\n",
      "657\n",
      "132\n",
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  7.63it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "\n",
    "agent = QAgent(lr = 1e-3, a_size = 1118)\n",
    "data = QData()\n",
    "\n",
    "replay_times = 10\n",
    "episodes = 10\n",
    "epsilon = 1\n",
    "train_epoch = 5\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    rewards = 0\n",
    "    losses = 0\n",
    "    for j in range(replay_times):\n",
    "        count = 0\n",
    "        end = False\n",
    "        g = init_graph\n",
    "        while(count < 10 and not end):\n",
    "            dgl_g = g.to_dgl_graph()\n",
    "            count += 1 \n",
    "            node, A = agent.select_a(dgl_g, epsilon)\n",
    "            # print(A)\n",
    "            new_g = g.apply_xfer(xfer=quartz_context.get_xfer_from_id(id=A), node = g.all_nodes()[node])\n",
    "            \n",
    "            if new_g == None:\n",
    "                end = True\n",
    "                data.add_data([dgl_g, torch.tensor(node), torch.tensor(A), torch.tensor(-1), None])\n",
    "            \n",
    "            else:\n",
    "                dgl_new_g = new_g.to_dgl_graph()\n",
    "                reward = g.num_gates() - new_g.num_gates()\n",
    "                                         \n",
    "                data.add_data([dgl_g, torch.tensor(node), torch.tensor(A), torch.tensor(reward), dgl_new_g])\n",
    "            \n",
    "                g = new_g\n",
    "                rewards += reward\n",
    "        \n",
    "\n",
    "    for j in range(train_epoch):\n",
    "        loss = agent.train(data, 3)\n",
    "        losses += loss  \n",
    "        \n",
    "    if epsilon > 0.05 :\n",
    "        epsilon -= 0.0001\n",
    "        \n",
    "\n",
    "    agent.target_net.load_state_dict(agent.q_net.state_dict())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
